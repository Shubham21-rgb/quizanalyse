"""
LangChain-Based AI Agent Architecture for Quiz Solving
Robust implementation with Think â†’ Act â†’ Reflect paradigm
"""

import requests
import json
import re
import base64
from typing import Dict, List, Any, Optional, Type
from bs4 import BeautifulSoup
import pandas as pd
from io import StringIO

# LangChain imports
from langchain.tools import BaseTool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.language_models import BaseChatModel
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field
import traceback


class CustomLLM(BaseChatModel):
    """Custom LangChain LLM wrapper for AIPipe API"""
    
    api_key: str
    base_url: str = "https://aipipe.org/openai/v1"
    model_name: str = "gpt-4o-mini"
    temperature: float = 0.7
    
    @property
    def _llm_type(self) -> str:
        return "custom_aipipe"
    
    def bind_tools(self, tools: list, **kwargs: Any) -> "CustomLLM":
        """Bind tools to the model (required for LangGraph agents)"""
        # Store tools for potential future use, but return self
        # LangGraph will handle tool execution separately
        return self
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Any:
        """Generate response from AIPipe API"""
        
        # Convert LangChain messages to OpenAI format
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                formatted_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                formatted_messages.append({"role": "assistant", "content": msg.content})
            elif isinstance(msg, SystemMessage):
                formatted_messages.append({"role": "system", "content": msg.content})
        
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": formatted_messages,
            "temperature": kwargs.get('temperature', self.temperature)
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=120)
            response.raise_for_status()
            data = response.json()
            
            content = data['choices'][0]['message']['content']
            
            # Return in LangChain format
            from langchain_core.outputs import ChatGeneration, ChatResult
            message = AIMessage(content=content)
            generation = ChatGeneration(message=message)
            return ChatResult(generations=[generation])
            
        except Exception as e:
            print(f"âŒ LLM call failed: {e}")
            from langchain_core.outputs import ChatGeneration, ChatResult
            message = AIMessage(content=f"Error: {str(e)}")
            generation = ChatGeneration(message=message)
            return ChatResult(generations=[generation])


class WebScraperInput(BaseModel):
    """Input schema for web scraper tool"""
    url: str = Field(description="URL to scrape")


class WebScraperTool(BaseTool):
    """LangChain tool for web scraping"""
    
    name: str = "web_scraper"
    description: str = """Useful for scraping web pages to extract data. 
    Input should be a URL string.
    Returns HTML content and extracted text."""
    args_schema: Type[BaseModel] = WebScraperInput
    
    def _run(self, url: str) -> str:
        """Execute web scraping"""
        try:
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            response = requests.get(url, headers=headers, timeout=30)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extract text content
            text = soup.get_text(separator='\n', strip=True)
            
            result = {
                "status": "success",
                "status_code": response.status_code,
                "url": url,
                "text_preview": text[:2000],
                "html_preview": response.text[:1000]
            }
            
            return json.dumps(result, indent=2)
            
        except Exception as e:
            return json.dumps({
                "status": "error",
                "error": str(e),
                "url": url
            })
    
    async def _arun(self, url: str) -> str:
        """Async version"""
        return self._run(url)


class DataProcessorInput(BaseModel):
    """Input schema for data processor tool"""
    data: str = Field(description="Data to process (CSV, JSON, or text)")
    operation: str = Field(description="Operation: parse_csv, parse_json, calculate_stats, decode_base64")


class DataProcessorTool(BaseTool):
    """LangChain tool for data processing"""
    
    name: str = "data_processor"
    description: str = """Useful for processing and analyzing data.
    Supports operations: parse_csv, parse_json, calculate_stats, decode_base64.
    Input should be data string and operation name."""
    args_schema: Type[BaseModel] = DataProcessorInput
    
    def _run(self, data: str, operation: str) -> str:
        """Execute data processing"""
        try:
            if operation == "parse_csv":
                df = pd.read_csv(StringIO(data))
                result = {
                    "status": "success",
                    "operation": "parse_csv",
                    "shape": df.shape,
                    "columns": df.columns.tolist(),
                    "head": df.head(10).to_dict(),
                    "dtypes": df.dtypes.astype(str).to_dict()
                }
                return json.dumps(result, indent=2)
                
            elif operation == "parse_json":
                parsed = json.loads(data)
                result = {
                    "status": "success",
                    "operation": "parse_json",
                    "data": parsed
                }
                return json.dumps(result, indent=2)
                
            elif operation == "calculate_stats":
                df = pd.read_csv(StringIO(data))
                result = {
                    "status": "success",
                    "operation": "calculate_stats",
                    "statistics": df.describe().to_dict(),
                    "null_counts": df.isnull().sum().to_dict()
                }
                return json.dumps(result, indent=2)
                
            elif operation == "decode_base64":
                decoded = base64.b64decode(data).decode('utf-8')
                result = {
                    "status": "success",
                    "operation": "decode_base64",
                    "decoded_data": decoded[:1000]
                }
                return json.dumps(result, indent=2)
            
            return json.dumps({
                "status": "error",
                "error": f"Unknown operation: {operation}"
            })
            
        except Exception as e:
            return json.dumps({
                "status": "error",
                "operation": operation,
                "error": str(e)
            })
    
    async def _arun(self, data: str, operation: str) -> str:
        """Async version"""
        return self._run(data, operation)


class QuizAnalyzerInput(BaseModel):
    """Input schema for quiz analyzer tool"""
    content: str = Field(description="Quiz page content to analyze")


class QuizAnalyzerTool(BaseTool):
    """LangChain tool for analyzing quiz content"""
    
    name: str = "quiz_analyzer"
    description: str = """Useful for analyzing quiz page content.
    Extracts questions, instructions, data sources, and submission URLs.
    Input should be the HTML or text content of the quiz page."""
    args_schema: Type[BaseModel] = QuizAnalyzerInput
    
    def _run(self, content: str) -> str:
        """Analyze quiz content"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # Extract sections
            sections = {}
            for i in range(1, 10):
                section_header = soup.find(['h2', 'h3'], string=re.compile(f'SECTION {i}', re.I))
                if section_header:
                    section_content = []
                    for sibling in section_header.find_next_siblings():
                        if sibling.name in ['h2', 'h3'] and 'SECTION' in sibling.get_text():
                            break
                        section_content.append(sibling.get_text(strip=True))
                    sections[f'section_{i}'] = '\n'.join(section_content)
            
            # Find submission URL
            submit_url = None
            for link in soup.find_all('a'):
                href = link.get('href', '')
                if 'submit' in href.lower() or 'answer' in href.lower():
                    submit_url = href
                    break
            
            # Extract data URLs
            data_urls = []
            for link in soup.find_all('a'):
                href = link.get('href', '')
                if any(ext in href for ext in ['.csv', '.json', '.txt', '.pdf', '.xlsx']):
                    data_urls.append(href)
            
            result = {
                "status": "success",
                "sections_found": len(sections),
                "sections": sections,
                "submit_url": submit_url,
                "data_urls": data_urls,
                "page_title": soup.title.string if soup.title else None
            }
            
            return json.dumps(result, indent=2)
            
        except Exception as e:
            return json.dumps({
                "status": "error",
                "error": str(e)
            })
    
    async def _arun(self, content: str) -> str:
        """Async version"""
        return self._run(content)


class LangChainQuizAgent:
    """
    Main LangChain-based AI Agent for quiz solving
    Implements robust Think â†’ Act â†’ Reflect paradigm
    """
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        
        # Initialize custom LLM
        self.llm = CustomLLM(api_key=api_key, temperature=0.4)
        
        # Initialize tools
        self.tools = [
            WebScraperTool(),
            DataProcessorTool(),
            QuizAnalyzerTool()
        ]
        
        # Create tools dict for easy lookup
        self.tools_dict = {tool.name: tool for tool in self.tools}
        
        # Initialize memory
        self.conversation_history = []
        self.working_memory = {}
        self.iteration_count = 0
    
    def _create_system_prompt(self) -> str:
        """Create the system prompt for the agent"""
        
        tools_desc = "\n".join([f"- {tool.name}: {tool.description}" for tool in self.tools])
        
        return f"""You are an expert AI agent specialized in solving quiz challenges involving data analysis, web scraping, and computation.

AVAILABLE TOOLS:
{tools_desc}

QUIZ SOLVING STRATEGY:

1. **UNDERSTAND THE TASK** (Read SECTION 1 + Audio Transcriptions)
   - SECTION 1 contains the MAIN TASK and instructions
   - For AUDIO-BASED TASKS: ALSO check SECTION 5 for audio transcriptions
   - Audio transcriptions provide CRITICAL context about what to do
   - Combine text instructions with audio content for complete understanding
   - Extract what needs to be done: scrape data, calculate, decode, etc.
   - Identify the EXPECTED JSON FORMAT for submission (shown in code blocks)
   - Find submission endpoint (IMPORTANT: Usually BASE_URL + /submit!)

2. **EXTRACT URLs**
   - Base URL: Look for "Original URL" in Page Metadata section
   - Extract scheme + domain (e.g., https://tds-llm-analysis.s-anand.net)
   - Convert ALL relative URLs to absolute: BASE_URL + relative_path
   - Example: /demo-scrape-data â†’ https://domain.com/demo-scrape-data
   - Find data source URLs from SECTION 1 or SECTION 2

ğŸ¯ **CRITICAL SUBMISSION URL PATTERN:**
   - SUBMIT_URL = BASE_URL + "/submit" (submission endpoint)
   - Example: Base URL = https://domain.com â†’ SUBMIT_URL = https://domain.com/submit
   - The submission endpoint is typically /submit

3. **EXTRACT PARAMETERS**
   - Check "URL Query Parameters" section for email, id, secret, etc.
   - Check REQUEST BODY DATA section for secret, email
   - These values MUST be included in submission payload

4. **SCRAPE/FETCH DATA**
   - Use the absolute data URL to fetch content
   - Parse HTML with BeautifulSoup
   - Extract the required information (text, code, number, table data)
   - Handle base64 decoding if needed (look for base64 strings in HTML)

ğŸµ **AUDIO TASK SPECIAL HANDLING:**
   - If SECTION 5 has audio transcriptions, READ THEM CAREFULLY
   - Audio transcriptions contain the actual task instructions
   - Example: "you need to download the csv file provided pick the first column and add all values greater than or equal to the cutoff value"
   - Use transcription + SECTION 1 text to understand the complete task
   - The transcription often clarifies what calculation or processing to do

ğŸ”¢ **CUTOFF VALUE CALCULATION FOR AUDIO TASKS:**
   - If you see "cutoff" mentioned in audio transcription or HTML
   - The cutoff is calculated by emailNumber() function = SHA1(email)[:4] converted to int
   - For email "23f2003481@ds.study.iitm.ac.in", cutoff = 23109
   - NEVER try to scrape dynamic JS content - calculate directly from email
   - Use: `import hashlib; cutoff = int(hashlib.sha1(email.encode()).hexdigest()[:4], 16)`

5. **GENERATE SOLUTION CODE**

ğŸ”§ **MANDATORY IMPORTS FOR CSV TASKS:**
   - When working with CSV data, ALWAYS include: `from io import StringIO`
   - Required for: `pd.read_csv(StringIO(response.text))`
   - Missing StringIO import will crash the code!
   
ğŸš¨ **MANDATORY ERROR HANDLING - MUST FOLLOW:**

âŒ **NEVER write code like this (CRASHES):**
```python
elem = soup.find('div').get_text()        # âŒ Crashes if None
code = text.split('=')[1]                  # âŒ Crashes if no '='
value = lines[1]                           # âŒ Crashes if len < 2
```

âœ… **ALWAYS write code like this (SAFE):**
```python
elem = soup.find('div')
if elem:                                   # âœ… Check for None!
    text = elem.get_text()
    
if '=' in text:                           # âœ… Check before split
    parts = text.split('=')
    if len(parts) > 1:                    # âœ… Check length!
        value = parts[1]

lines = text.splitlines()
if len(lines) > 1:                        # âœ… Check length!
    value = lines[1]
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš¨ CRITICAL INSTRUCTION - READ CAREFULLY ğŸš¨
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

YOUR TASK: Copy the template below EXACTLY, character by character.

âŒ DO NOT:
- Write your own scraping function
- Simplify or skip ANY strategy
- Remove Strategy 2 (external JS fetching)
- Remove any imports (re, hashlib, base64 are ALL required)

âœ… ONLY CHANGE:
- Line 10: BASE_URL = "..." (use the actual base URL from context)
- Line 11: DATA_URL = "..." (use the actual data URL)
- Line 12: SUBMIT_URL = "..." (use the actual submit URL)
- Line 13: EMAIL = "..." (use the actual email from context)
- Line 14: SECRET = "..." (use the actual secret from context)
- Line 15: URL_PARAM = "..." (use the original quiz URL)

âš ï¸ IMPORTANT: When you see <div id="question"></div> with <script src="demo-scrape.js">,
   the content is loaded by JavaScript! Strategy 2 will fetch demo-scrape.js,
   detect emailNumber(), and calculate the answer from email SHA1.
   DO NOT SKIP Strategy 2 - it's critical for JavaScript-loaded content!

THE TEMPLATE INCLUDES ALL STRATEGIES:
âœ… Strategy 1: Pattern matching ("Secret code is 23109")
âœ… Strategy 2: External JS fetching + email SHA1 (CRITICAL!)
âœ… Strategy 3: Base64 decoding from inline scripts

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ YOUR RESPONSE MUST BE THE CODE BELOW WITH URLS UPDATED âš ï¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1: Copy the ENTIRE code block below
Step 2: Replace ONLY these 6 lines:
  - Line 10: BASE_URL = "..." â†’ Use actual base URL
  - Line 11: DATA_URL = "..." â†’ Use actual data URL  
  - Line 12: SUBMIT_URL = "..." â†’ Use QUIZ URL (same as Original URL!)
  - Line 13: EMAIL = "..." â†’ Use actual email
  - Line 14: SECRET = "..." â†’ Use actual secret
  - Line 15: URL_PARAM = "..." â†’ Use original quiz URL
Step 3: Return the complete code in ```python ``` block

DO NOT modify the scrape_data() function!
DO NOT remove or change any strategy!

```python
import requests
from bs4 import BeautifulSoup
import json
import base64
import re
import hashlib
import pandas as pd
from io import StringIO

# 1. DEFINE CONSTANTS (extracted from quiz context)
BASE_URL = "https://..."  # From Original URL
DATA_URL = BASE_URL + "/path"  # Absolute URL
SUBMIT_URL = "https://..."  # BASE_URL + /submit
EMAIL = "..."  # From URL params or request body
SECRET = "..."  # From request body
URL_PARAM = "..."  # Original quiz URL

# 2. SCRAPE DATA FUNCTION (ROBUST - with error handling)
def scrape_data():
    \"\"\"Extract data with multiple fallback strategies. ALWAYS check for None!\"\"\"
    headers = {{"User-Agent": "Mozilla/5.0"}}
    
    try:
        response = requests.get(DATA_URL, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        print(f"âœ“ Fetched {{DATA_URL}}")
    except Exception as e:
        print(f"âŒ Fetch error: {{e}}")
        return None
    
    # Strategy 1: Common element IDs - ALWAYS check for None!
    try:
        for elem_id in ['secret', 'code', 'answer', 'result', 'data', 'question']:
            elem = soup.find(id=elem_id)
            if elem:  # âœ… Check for None!
                text = elem.get_text().strip()
                if text:
                    # Pattern: "Secret code is 23109" or similar
                    import re
                    # Look for numbers after "code is", "secret is", "answer is"
                    patterns = [
                        r'(?:secret|code|answer)\\s+is\\s+(\\d+)',
                        r'(?:secret|code|answer):\\s*(\\d+)',
                        r'(?:secret|code|answer)\\s*=\\s*(\\d+)',
                        r'<strong>(\\d+)</strong>',
                        r'\\b(\\d{{4,}})\\b'  # Any 4+ digit number
                    ]
                    for pattern in patterns:
                        match = re.search(pattern, text, re.IGNORECASE)
                        if match:
                            value = match.group(1)
                            print(f"âœ“ Extracted by pattern: {{value}}")
                            return value
                    # Fallback: return first line if short enough
                    lines = text.splitlines()
                    if len(lines) > 0 and len(lines[0]) < 100:
                        print(f"âœ“ Found by ID: {{lines[0]}}")
                        return lines[0]
    except Exception as e:
        print(f"âš  Strategy 1 error: {{e}}")
    
    # Strategy 2: CRITICAL for JS-loaded content!
    # When HTML has <div id="question"></div> (empty) + <script src="...">,
    # the content is loaded by JavaScript. Fetch the JS file and calculate answer.
    try:
        for script_tag in soup.find_all('script', src=True):
            js_url = script_tag.get('src', '')
            if js_url:
                if not js_url.startswith('http'):
                    js_url = BASE_URL + ('/' if not js_url.startswith('/') else '') + js_url
                print(f"â†’ Fetching external JS: {{js_url}}")
                try:
                    js_resp = requests.get(js_url, headers=headers, timeout=30)
                    js_content = js_resp.text
                    # Check if it uses emailNumber or SHA1 (common pattern)
                    if 'emailNumber' in js_content or 'sha1' in js_content.lower() or 'utils.js' in js_content:
                        print(f"âœ“ Detected email-based calculation")
                        sha1_hash = hashlib.sha1(EMAIL.encode()).hexdigest()
                        number = int(sha1_hash[:4], 16)
                        print(f"âœ“ Calculated from email: {{number}}")
                        return str(number)
                except:
                    pass
    except Exception as e:
        print(f"âš  Strategy 2 error: {{e}}")
    
    # Strategy 3: Check inline scripts for base64/variables
    try:
        for script in soup.find_all('script'):
            if script.string:  # âœ… Check if script has content!
                # Look for base64 (20+ chars of A-Za-z0-9+/=)
                b64_matches = re.findall(r'([A-Za-z0-9+/]{{20,}}={{0,2}})', script.string)
                for b64 in b64_matches:
                    try:
                        decoded = base64.b64decode(b64).decode('utf-8')
                        if 5 < len(decoded) < 200:
                            print(f"âœ“ Decoded: {{decoded}}")
                            return decoded.strip()
                    except:
                        pass
    except Exception as e:
        print(f"âš  Strategy 3 error: {{e}}")
    
    print(f"âŒ No data found. HTML:\\n{{response.text[:500]}}")
    return None

# 3. SUBMIT ANSWER FUNCTION (with retry and type conversion)
def submit_answer(answer, max_retries=3):
    \"\"\"Submit answer with automatic retries and type conversion\"\"\"
    
    # Convert answer to JSON-serializable type
    if hasattr(answer, 'item'):  # Handle pandas/numpy types
        answer = answer.item()
    elif hasattr(answer, 'tolist'):  # Handle numpy arrays
        answer = answer.tolist()
    elif hasattr(answer, 'to_dict'):  # Handle pandas objects
        answer = answer.to_dict()
    
    # Ensure it's a basic Python type
    import json
    try:
        json.dumps(answer)  # Test if it's serializable
    except (TypeError, ValueError):
        # Convert to string as fallback
        answer = str(answer)
    
    payload = {{
        "email": EMAIL,
        "secret": SECRET,
        "url": URL_PARAM,
        "answer": answer
    }}
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ“¤ Submission attempt {{attempt + 1}}/{{max_retries}}")
            response = requests.post(SUBMIT_URL, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            print(f"ğŸ“¥ Submission response: {{result}}")
            return result
        except Exception as e:
            print(f"âŒ Submission error (attempt {{attempt + 1}}): {{e}}")
            if attempt < max_retries - 1:
                print(f"ğŸ”„ Retrying in {{attempt + 1}} seconds...")
                import time
                time.sleep(attempt + 1)
            else:
                print("âŒ All retry attempts failed")
                return {{"error": f"Submission failed: {{str(e)}}"}}
    
    return {{"error": "Max retries exceeded"}}

# 4. MAIN EXECUTION
if __name__ == "__main__":
    try:
        data = scrape_data()
        print(f"âœ… Scraped data: {{data}}")
        result = submit_answer(data)
        print(f"âœ… Final result: {{result}}")
    except Exception as e:
        print(f"âŒ Error: {{e}}")
```

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š ALTERNATIVE TEMPLATES (For non-scraping tasks)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**For Data Processing/Calculation:**
```python
import requests
import pandas as pd
from io import StringIO

BASE_URL = "https://example.com"
DATA_URL = BASE_URL + "/data.csv"
SUBMIT_URL = BASE_URL + "/submit"  # Submission endpoint
EMAIL = "email@example.com"
SECRET = "secret"
ORIGINAL_URL = "quiz_url"

def process_data():
    df = pd.read_csv(DATA_URL)
    result = df['column'].mean()  # Or any calculation
    return str(result)

def submit_answer(answer, max_retries=3):
    \"\"\"Submit answer with automatic retries and type conversion\"\"\"
    # Convert answer to JSON-serializable type
    if hasattr(answer, 'item'):  # pandas/numpy types
        answer = answer.item()
    elif hasattr(answer, 'tolist'):  # numpy arrays
        answer = answer.tolist()
    
    import json
    try:
        json.dumps(answer)  # Test serialization
    except (TypeError, ValueError):
        answer = str(answer)  # Convert to string as fallback
    
    payload = {{"email": EMAIL, "secret": SECRET, "url": ORIGINAL_URL, "answer": answer}}
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ“¤ Submission attempt {{attempt + 1}}/{{max_retries}}")
            response = requests.post(SUBMIT_URL, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            print(f"ğŸ“¥ Response: {{result}}")
            return result
        except Exception as e:
            print(f"âŒ Submission error: {{e}}")
            if attempt < max_retries - 1:
                import time
                time.sleep(attempt + 1)
    return {{"error": "Submission failed"}}

if __name__ == "__main__":
    answer = process_data()
    print(f"âœ“ Calculated result: {{answer}}")
    print(submit_answer(answer))
```

**For API Request:**
```python
import requests

BASE_URL = "https://example.com"
API_URL = BASE_URL + "/api/endpoint"
SUBMIT_URL = BASE_URL + "/submit"  # Submission endpoint
EMAIL = "email@example.com"
SECRET = "secret"
ORIGINAL_URL = "quiz_url"

def fetch_api_data():
    response = requests.get(API_URL, timeout=30)
    data = response.json()
    # Process as needed
    return data['key']  # Or extract specific field

def submit_answer(answer, max_retries=3):
    \"\"\"Submit answer with automatic retries and type conversion\"\"\"
    # Handle different data types
    if hasattr(answer, 'item'):  # pandas/numpy types
        answer = answer.item()
    elif hasattr(answer, 'tolist'):  # numpy arrays
        answer = answer.tolist()
    
    import json
    try:
        json.dumps(answer)  # Test serialization
    except (TypeError, ValueError):
        answer = str(answer)  # Fallback to string
    
    payload = {{"email": EMAIL, "secret": SECRET, "url": ORIGINAL_URL, "answer": answer}}
    
    for attempt in range(max_retries):
        try:
            print(f"ğŸ“¤ Attempt {{attempt + 1}}: {{answer}}")
            response = requests.post(SUBMIT_URL, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            print(f"ğŸ“¥ Response: {{result}}")
            return result
        except Exception as e:
            print(f"âŒ Error: {{e}}")
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
    return {{"error": "Failed after retries"}}

if __name__ == "__main__":
    answer = fetch_data()
    print(f"âœ“ Fetched result: {{answer}}")
    print(submit_answer(answer))
```

ğŸ”’ CRITICAL REQUIREMENTS (MANDATORY - MUST FOLLOW ALL):
âœ… Use ABSOLUTE URLs (not /relative/path)
âœ… Extract BASE_URL from "Original URL" in metadata
âœ… Get email/id from "URL Query Parameters" section
âœ… Get secret from "REQUEST BODY DATA" section if present
âœ… Match EXACT JSON format shown in SECTION 1
âœ… **ALWAYS check if soup.find() returns None before calling methods on it**
âœ… **ALWAYS check length before using list[index] or splitlines()[index]**
âœ… **WRAP each extraction attempt in try-except**
âœ… **Have MULTIPLE fallback strategies (don't rely on one)**
âœ… Print all debug info (URLs, scraped data, response)
âœ… Include User-Agent header in all requests
âœ… Add timeout=30 to all requests
âœ… Handle errors gracefully - continue to next strategy if one fails

âŒ FORBIDDEN PATTERNS (These WILL crash your code):
- soup.find('div').get_text() â†’ Must check if elem is not None first!
- text.split('=')[1] â†’ Must check if '=' exists and len(parts) > 1!
- lines[1] or splitlines()[1] â†’ Must check if len(lines) > 1!
- script_tag = soup.find('script') â†’ Only checks ONE inline script, misses external JS!

ğŸ¯ VALIDATION CHECK - Your generated code MUST contain:
   1. for script_tag in soup.find_all('script', src=True):
   2. sha1_hash = hashlib.sha1(EMAIL.encode()).hexdigest()
   3. int(sha1_hash[:4], 16)
   
   This is CRITICAL for JavaScript-loaded content. If the HTML contains:
   <div id="question"></div>
   <script src="demo-scrape.js"></script>
   
   Then requests.get() will see an EMPTY div, but the JS file calculates the answer.
   Strategy 2 (lines 50-72 in template) handles this by:
   1. Finding <script src="..."> tags with soup.find_all('script', src=True)
   2. Fetching each external JS file
   3. Detecting 'emailNumber' or 'sha1' in the JS code
   4. Calculating SHA1(email)[:4] converted to int â†’ the answer
   
   âš ï¸ CRITICAL: Use simple regex patterns without escaped quotes!
   âŒ WRONG: r'[\"\']([A-Za-z0-9+/]{{20,}})[\"\']'  (syntax error!)
   âœ… RIGHT: r'([A-Za-z0-9+/]{{20,}}={{0,2}})'  (works!)
   
   WITHOUT these lines, you'll get empty/wrong answers on JS-loaded pages!

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ STEP 1: ANALYZE TASK TYPE (CRITICAL - DO THIS FIRST!)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ NOT ALL TASKS REQUIRE SCRAPING! Read SECTION 1 carefully to determine task type:

ğŸ“‹ TASK TYPE IDENTIFICATION:

1. **SIMPLE SUBMISSION** (No scraping needed!)
   - Keywords: "POST this JSON", "submit anything", "answer: anything you want"
   - Example: "POST {{email, secret, url, answer: 'anything you want'}}"
   - Action: Just submit the JSON with a simple answer like "test" or "hello"
   - DO NOT scrape if instructions say "anything you want"!

2. **SCRAPING TASK** (Fetch external data)
   - Keywords: "scrape", "fetch data from URL", "get the secret code from"
   - Example: "Scrape /demo-scrape-data?email=... Get the secret code"
   - Action: Use scraping strategies to extract data from specified URL

3. **CALCULATION TASK** (Mathematical operations)
   - Keywords: "calculate", "sum", "average", "compute", "find the result"
   - Example: "Calculate the sum of values in the table"
   - Action: Extract numbers and perform calculations

4. **API TASK** (Call external API)
   - Keywords: "API", "endpoint", "fetch from API", "call the service"
   - Example: "Call the API at /api/data and process the response"
   - Action: Make HTTP requests to API endpoints

5. **DATA PROCESSING** (Transform/analyze data)
   - Keywords: "process", "transform", "filter", "analyze", "clean"
   - Example: "Process the CSV data and return the top 10 rows"
   - Action: Load, clean, and transform data

6. **VISUALIZATION** (Generate charts/images)
   - Keywords: "visualize", "chart", "graph", "plot", "generate image"
   - Example: "Create a bar chart showing the distribution"
   - Action: Generate images using matplotlib/plotly and encode as base64

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ AVAILABLE HELPER: LLMFunc.py (OPTIONAL)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

You have access to LLMFunc.py's LLMScraperHandler class which provides:
- Intelligent web scraping with retry logic and connection pooling
- Automatic content type detection (JSON, CSV, HTML, etc.)
- Handles both static and JavaScript-loaded pages
- Returns formatted markdown reports with structured data

You can import and use it in your code if needed:
```python
from LLMFunc import LLMScraperHandler
scraper = LLMScraperHandler()
result = scraper.scrape_url(url)
```

However, for most scraping tasks, the template's built-in strategies
(requests + BeautifulSoup) are sufficient. Use LLMFunc only if you need
advanced features like JavaScript execution or complex content parsing.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ YOUR RESPONSE MUST FOLLOW THIS EXACT FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Part 1: TASK ANALYSIS (CRITICAL - Analyze before coding!):

1. **Task Type Identification:**
   - [ ] Web Scraping (extract data from HTML/JS)
   - [ ] Data Processing (calculation, transformation)
   - [ ] API Call (fetch from endpoint)
   - [ ] File Analysis (CSV, JSON parsing)
   - [ ] Visualization (chart, graph generation)
   - [ ] Other: [specify]

2. **What SECTION 1 asks for:** [describe the requirement]

3. **Data Source:**
   - If scraping: DATA_URL = [url to scrape]
   - If API: API_ENDPOINT = [endpoint]
   - If calculation: INPUT_DATA = [where to get data]
   - If file: FILE_URL = [file location]

4. **Approach:**
   - Method: [scraping/calculation/API call/etc.]
   - Tools needed: [requests/pandas/matplotlib/etc.]
   - Strategy: [how to solve]

Part 2: Complete Python code

ğŸ” **Choose the RIGHT template based on task type:**

**For Web Scraping Tasks:**
- Use the scraping template above (with all strategies)
- Change only URLs and credentials (lines 10-15)
- Keep ALL strategies including Strategy 2 (external JS)

**For Calculation/Processing Tasks:**
- Import necessary libraries (pandas, numpy, etc.)
- Fetch data if needed
- Perform calculations
- Format result
- Submit answer

**For Audio Cutoff Tasks (CSV filtering):**
```python
import requests
import pandas as pd
import hashlib
from io import StringIO

BASE_URL = "https://example.com"
CSV_URL = BASE_URL + "/data.csv"
SUBMIT_URL = BASE_URL + "/submit"  # Submission endpoint
EMAIL = "email@example.com"
SECRET = "secret"
ORIGINAL_URL = "quiz_url"

def process_audio_cutoff_task():
    # Calculate cutoff from email (standard pattern)
    cutoff = int(hashlib.sha1(EMAIL.encode()).hexdigest()[:4], 16)
    print(f"âœ“ Calculated cutoff: {{cutoff}}")
    
    # Download CSV
    response = requests.get(CSV_URL, timeout=30)
    df = pd.read_csv(StringIO(response.text))
    print(f"âœ“ Loaded CSV with shape: {{df.shape}}")
    
    # Filter first column >= cutoff and sum
    first_col = df.iloc[:, 0]
    filtered_values = first_col[first_col >= cutoff]
    result = filtered_values.sum()
    print(f"âœ“ Sum of values >= {{cutoff}}: {{result}}")
    return result

# Submit with retry logic (same as other templates)
def submit_answer(answer, max_retries=3):
    if hasattr(answer, 'item'):
        answer = answer.item()
    
    payload = {{"email": EMAIL, "secret": SECRET, "url": ORIGINAL_URL, "answer": answer}}
    
    for attempt in range(max_retries):
        try:
            response = requests.post(SUBMIT_URL, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            if attempt < max_retries - 1:
                import time
                time.sleep(1)
    return {{"error": "Failed"}}

if __name__ == "__main__":
    answer = process_audio_cutoff_task()
    print(submit_answer(answer))
```

**For API/File Tasks:**
- Use requests.get() for API endpoints
- Use pandas.read_csv() or json.loads() for files
- Process the data as needed
- Submit result

**For Visualization Tasks:**
- Use matplotlib/seaborn
- Generate chart and save to file
- Return file path or base64 encoded image

Wrap code in ```python ``` markers.

ğŸ“ **TASK TYPE EXAMPLES:**

â€¢ "Scrape the data from /demo-scrape-data" â†’ Web Scraping Task
â€¢ "Calculate the average of column X" â†’ Data Processing Task
â€¢ "Fetch data from /api/data and return top 5" â†’ API + Processing
â€¢ "Parse the CSV at /data.csv" â†’ File Analysis Task
â€¢ "Create a bar chart showing..." â†’ Visualization Task

âš ï¸ **For Scraping Tasks:** If HTML shows <div id="question"></div> + <script src="...js">,
   the content is JS-loaded! Strategy 2 (lines 50-72 in template) will:
   1. Find <script src="demo-scrape.js">
   2. Fetch https://domain.com/demo-scrape.js
   3. Detect 'emailNumber' in the JS
   4. Calculate SHA1(email)[:4] â†’ answer
   This strategy is CRITICAL for scraping - do not remove it!
"""
    
    def _use_tool(self, tool_name: str, tool_input: str) -> str:
        """Execute a tool and return the result"""
        if tool_name not in self.tools_dict:
            return f"Error: Tool '{tool_name}' not found. Available tools: {list(self.tools_dict.keys())}"
        
        tool = self.tools_dict[tool_name]
        try:
            print(f"\nğŸ”§ Using tool: {tool_name}")
            print(f"ğŸ“¥ Input: {tool_input[:200]}...")
            result = tool._run(tool_input)
            print(f"âœ… Tool execution successful")
            return result
        except Exception as e:
            error_msg = f"Tool execution error: {str(e)}\n{traceback.format_exc()}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def solve_quiz(self, quiz_context: str, max_attempts: int = 3) -> str:
        """
        Main method to solve a quiz using LangChain agent
        
        Args:
            quiz_context: Full context about the quiz task
            max_attempts: Maximum retry attempts
            
        Returns:
            Generated solution code
        """
        
        print(f"\n{'ğŸ¤–'*35}")
        print(f"{'ğŸ¤–'*10} LANGCHAIN AGENT ACTIVATED {'ğŸ¤–'*10}")
        print(f"{'ğŸ¤–'*35}\n")
        
        for attempt in range(max_attempts):
            self.iteration_count = attempt + 1
            
            print(f"\n{'â”'*70}")
            print(f"ğŸ“ AGENT ATTEMPT {attempt + 1}/{max_attempts}")
            print(f"{'â”'*70}\n")
            
            try:
                # Construct messages for LangChain
                system_message = SystemMessage(content=self._create_system_prompt())
                user_message = HumanMessage(content=f"""
QUIZ CHALLENGE:
{quiz_context}

YOUR TASK:
1. Analyze the quiz page content thoroughly
2. Identify the question in SECTION 1
3. Determine what data needs to be extracted or calculated
4. Generate complete Python solution code that:
   - Solves the quiz task
   - Extracts the submission URL from the quiz page
   - Submits the answer to the correct endpoint
   - Handles errors gracefully
   - Prints debug information and submission response

First explain your reasoning, then provide the complete executable Python code.
""")
                
                # Add conversation history
                messages = [system_message]
                for hist_msg in self.conversation_history:
                    messages.append(hist_msg)
                messages.append(user_message)
                
                # Call LLM
                print(f"ğŸ§  Calling LLM for reasoning and code generation...")
                response = self.llm._generate(
                    messages=messages,
                    temperature=0.4 + (attempt * 0.1)
                )
                
                # Extract response content
                final_answer = response.generations[0].message.content
                
                print(f"\n{'='*70}")
                print(f"âœ… AGENT RESPONSE GENERATED")
                print(f"{'='*70}")
                print(f"Response length: {len(final_answer)} characters")
                
                # Store in conversation history
                self.conversation_history.append(user_message)
                self.conversation_history.append(AIMessage(content=final_answer))
                
                # Store in working memory
                self.working_memory[f'attempt_{attempt}'] = {
                    'input': quiz_context[:500],
                    'output': final_answer[:500],
                    'temperature': 0.4 + (attempt * 0.1)
                }
                
                return final_answer
                
            except Exception as e:
                print(f"\nâŒ Agent execution failed: {e}")
                print(f"Traceback: {traceback.format_exc()}")
                
                if attempt < max_attempts - 1:
                    print(f"ğŸ”„ Retrying with adjusted parameters...")
                    # Increase temperature for more creativity
                    self.llm.temperature = min(0.8, self.llm.temperature + 0.15)
                else:
                    print(f"âš ï¸ Max attempts reached")
                    return None
        
        return None
    
    def get_memory_summary(self) -> str:
        """Get summary of agent's memory and state"""
        summary = f"LangChain Agent Memory Summary:\n"
        summary += f"- Attempts completed: {self.iteration_count}\n"
        summary += f"- Tools available: {len(self.tools)}\n"
        summary += f"- Working memory items: {len(self.working_memory)}\n"
        summary += f"- Current temperature: {self.llm.temperature}\n"
        summary += f"- Conversation history: {len(self.conversation_history)} messages\n"
        
        return summary
    
    def reset(self):
        """Reset agent state for new quiz"""
        self.conversation_history = []
        self.working_memory = {}
        self.iteration_count = 0
        self.llm.temperature = 0.4
        print("ğŸ”„ Agent state reset")


def create_quiz_agent(api_key: str) -> LangChainQuizAgent:
    """Factory function to create a LangChain quiz agent"""
    return LangChainQuizAgent(api_key=api_key)
